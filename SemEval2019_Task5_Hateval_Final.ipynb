{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SemEval2019_Task5_Hateval_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxAnDySvxV8z",
        "colab_type": "text"
      },
      "source": [
        "# SemEval 2019 Task 5 - Shared Task on Multilingual Detection of Hate\n",
        "\n",
        "[CodaLab Competion website](https://competitions.codalab.org/competitions/19935)\n",
        "\n",
        "[Reference](https://www.aclweb.org/anthology/S19-2088/)\n",
        "\n",
        "## How to run\n",
        "\n",
        "Update the parameter in `TrainParameters` as needed to switch between different settings.\n",
        "\n",
        "## Author\n",
        "- Yonael Bekele\n",
        "- Michael Lin\n",
        "- Helen Zhao\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozG8hrCgfbJC",
        "colab_type": "text"
      },
      "source": [
        "### Notes\n",
        "We cached pre-trained models and stuffs on Google Drive, you will need access to our team drive to run the code.\n",
        "Also, this notebook is meant to be run in Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nux0BKu8XLXH",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries and utility methods\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oit2cVrp8RHr",
        "colab_type": "code",
        "outputId": "21edccf9-4a85-435c-c9ea-879a755db62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Extra libraries\n",
        "!pip install revtok # library to reverse text field in torchtext"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting revtok\n",
            "  Downloading https://files.pythonhosted.org/packages/83/36/ceaee3090850fe4940361110cae71091b113c720e4ced21660758da6ced1/revtok-0.0.3-py3-none-any.whl\n",
            "Installing collected packages: revtok\n",
            "Successfully installed revtok-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6Y7nUzU-nnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# built-in libs\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "# pytorch stuffs\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# sklearn\n",
        "import sklearn\n",
        "\n",
        "# numpy\n",
        "import numpy as np\n",
        "\n",
        "# colab utils\n",
        "from google.colab import drive\n",
        "\n",
        "# config\n",
        "PROJECT_ROOT_PATH = \"/content/gdrive/Shared drives/cmput497_hateval\"\n",
        "LANG = \"en\"\n",
        "SPACY_CONFIG = {\n",
        "    \"en\": {\n",
        "        \"path\": os.path.join(PROJECT_ROOT_PATH, \"en_core_web_lg-2.2.0/en_core_web_lg/en_core_web_lg-2.2.0\")\n",
        "    }\n",
        "}\n",
        "CKPT_PATH = {\n",
        "    \"en\": {\n",
        "        \"path\": os.path.join(PROJECT_ROOT_PATH, \"checkpoints/en/\")\n",
        "    }\n",
        "}\n",
        "OUTPUT_PATH = {\n",
        "    \"en\": {\n",
        "        \"path\": os.path.join(PROJECT_ROOT_PATH, \"output/en/\")\n",
        "    }\n",
        "}\n",
        "DEBUG = True\n",
        "\n",
        "# mount gdrive\n",
        "if not os.path.isdir(PROJECT_ROOT_PATH):\n",
        "    drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "# create output path\n",
        "if not os.path.isdir(OUTPUT_PATH[LANG][\"path\"]):\n",
        "    os.makedirs(OUTPUT_PATH[LANG][\"path\"])\n",
        "\n",
        "def tokenizer(text):\n",
        "    # https://stackoverflow.com/questions/13896056/how-to-remove-user-mentions-and-urls-in-a-tweet-string-using-python\n",
        "    # step 1 & 3: remove mentions and url\n",
        "    clean_row = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
        "\n",
        "    # step 5: extracting words from hashtags using pascal case\n",
        "    # regex expression: https://stackoverflow.com/questions/1128305/regex-for-pascalcased-words-aka-camelcased-with-leading-uppercase-letter - Nicolas Henneaux solution\n",
        "    pascal = re.findall(r\"[A-Z][a-z0-9]*[A-Z0-9][a-z0-9]+[A-Za-z0-9]*\", clean_row)\n",
        "    for p in pascal:\n",
        "        # p_text = p.replace('#','')\n",
        "        re_content = re.findall(\"[A-Z][^A-Z]*\", p)\n",
        "        extracted = \" \".join(re_content)\n",
        "        clean_row = re.sub(\"#\" + p, extracted, clean_row)\n",
        "    # step 2: remove punctuation + non-alphanumericals\n",
        "    final_clean = re.sub(r\"[^A-Za-z0-9\\']+\", \" \", clean_row)\n",
        "\n",
        "    # step 4: Contracting whitespace\n",
        "    tokens = word_tokenize(str(final_clean.strip().lower()))\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgvQkvPL7wUz",
        "colab_type": "code",
        "outputId": "f6b9ef5d-b5ae-41e7-81c2-ff1aebe9311c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W75gAm_Rmdps",
        "colab_type": "code",
        "outputId": "dadb959f-50c3-401b-eb16-489065ae168b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Python version:', sys.version)\n",
        "print(\"NLTK version {}\".format(nltk.__version__))\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print('Torch Text version:', torchtext.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version: 3.6.9 (default, Nov  7 2019, 10:44:02) \n",
            "[GCC 8.3.0]\n",
            "NLTK version 3.2.5\n",
            "PyTorch version 1.3.1\n",
            "Torch Text version: 0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybDfbgne4Hlp",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M22rXCx_4KGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data field\n",
        "ID = torchtext.data.Field(sequential=False, use_vocab=False)\n",
        "# TEXT.reverse() to get original text from encoded text\n",
        "TEXT = torchtext.data.ReversibleField(sequential=True, tokenize=tokenizer, include_lengths=True, lower=True)\n",
        "HS_LABEL = torchtext.data.Field(\n",
        "    sequential=False, use_vocab=False, is_target=True, pad_token=None, unk_token=None\n",
        ")\n",
        "TR_LABEL = torchtext.data.Field(\n",
        "    sequential=False, use_vocab=False, is_target=True, pad_token=None, unk_token=None\n",
        ")\n",
        "AG_LABEL = torchtext.data.Field(\n",
        "    sequential=False, use_vocab=False, is_target=True, pad_token=None, unk_token=None\n",
        ")\n",
        "\n",
        "class BatchGenerator:\n",
        "    def __init__(self, dl, x_var, y_vars):\n",
        "        self.dl, self.x_var, self.y_vars = (\n",
        "            dl,\n",
        "            x_var,\n",
        "            y_vars,\n",
        "        ) \n",
        "\n",
        "    def __iter__(self):\n",
        "        for batch in self.dl:\n",
        "            x = getattr(batch, self.x_var) \n",
        "            if self.y_vars is not None: \n",
        "                y = torch.cat(\n",
        "                    [getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1\n",
        "                ).float()\n",
        "            else:\n",
        "                y = torch.zeros((1))\n",
        "\n",
        "            yield (x[0], y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "def load_data(lang, device, config, label, use_pretrained=False):\n",
        "    train_data, validation_data, test_data = torchtext.data.TabularDataset.splits(\n",
        "        path=\"{}/dataset/\".format(PROJECT_ROOT_PATH),\n",
        "        train=\"hs-{}.tsv.train\".format(lang),\n",
        "        validation=\"hs-{}.tsv.dev\".format(lang),\n",
        "        test=\"hs-{}.tsv.test\".format(lang),\n",
        "        format=\"tsv\",\n",
        "        skip_header=True,\n",
        "        fields=[\n",
        "            (\"id\", ID),\n",
        "            (\"text\", TEXT),\n",
        "            (\"HS\", HS_LABEL),\n",
        "            (\"TR\", TR_LABEL),\n",
        "            (\"AG\", AG_LABEL),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    print(\"Data Summary: Train: {}, Validation: {}, Test: {}\".format(len(train_data), len(validation_data), len(test_data)))\n",
        "\n",
        "    if use_pretrained:\n",
        "        TEXT.build_vocab(train_data, vectors=\"glove.twitter.27B.200d\", vectors_cache=os.path.join(PROJECT_ROOT_PATH, \".vector_cache/\"))\n",
        "    else:\n",
        "        TEXT.build_vocab(train_data)\n",
        "    HS_LABEL.build_vocab(train_data)\n",
        "    AG_LABEL.build_vocab(train_data)\n",
        "    TR_LABEL.build_vocab(train_data)\n",
        "\n",
        "    train_dataloader, validation_dataloader = torchtext.data.BucketIterator.splits(\n",
        "        (train_data, validation_data),\n",
        "        batch_sizes=(config.batch_size, config.batch_size),\n",
        "        device=device,\n",
        "        sort_key=lambda x: len(x.text),\n",
        "        sort_within_batch=False,\n",
        "        repeat=False,\n",
        "    )\n",
        "    test_dataloader = torchtext.data.Iterator(test_data, batch_size=config.test_batch_size, device=device, sort=False, sort_within_batch=False, repeat=False)\n",
        "\n",
        "    train_iter = BatchGenerator(train_dataloader, \"text\", label)\n",
        "    validation_iter = BatchGenerator(validation_dataloader, \"text\", label)\n",
        "    test_iter = BatchGenerator(test_dataloader, \"text\", label)\n",
        "    \n",
        "    return train_iter, validation_iter, test_iter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxaG6DAcP8hB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl4ezdgkgusg",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDsQg7JZguCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HyperParameters(object):\n",
        "    def __init__(self):\n",
        "        self.learning_rate = 0.0005\n",
        "        self.num_epochs = 100\n",
        "        self.momentum = 0\n",
        "        self.batch_size = 16\n",
        "        self.test_batch_size = 32\n",
        "        self.embedding_dim = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mLt7gdVXxLY",
        "colab_type": "text"
      },
      "source": [
        "## NN Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8K-jGuZU-Su",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, batch_size, label=None):\n",
        "        super(Net, self).__init__()\n",
        "        print(\"Num embedding: {}, Embedding dim: {}, Batch size: {}\".format(num_embeddings, embedding_dim, batch_size))\n",
        "        self.batch_size = batch_size\n",
        "        self.embed = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, 64, num_layers=2, dropout=0.5, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=64, out_features=2)\n",
        "        if label != None:\n",
        "          self.set_initial_weights(label)\n",
        "        \n",
        "        # if pretrained_weights:\n",
        "        #     self.emb.weight.data.copy_(pretrained_vec)\n",
        "\n",
        "    def set_initial_weights(self, label):\n",
        "        if label[0] == \"HS\":\n",
        "          vocab = HS_LABEL.vocab\n",
        "        elif label[0] == \"TR\":\n",
        "          vocab = TR_LABEL.vocab\n",
        "        elif label[0] == \"AG\":\n",
        "          vocab = AG_LABEL.vocab\n",
        "\n",
        "        tensor = torch.tensor((1,64), dtype=torch.float)\n",
        "        weights = nn.Parameter(torch.cat((tensor.new_full((1,64), 1/vocab.freqs[\"0\"]), tensor.new_full((1,64), 1/vocab.freqs[\"1\"])), 0), requires_grad=True)\n",
        "        assert weights.shape == self.fc.weight.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.fc.weight = weights\n",
        "  \n",
        "    def forward(self, x):\n",
        "        x = x.transpose(0, 1)\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.embed(x)\n",
        "        x, (h_n, _) = self.lstm(x)\n",
        "        # get the hidden state of the last layer\n",
        "        x = self.fc(h_n[-1])\n",
        "        return x\n",
        "\n",
        "class NetGlove(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, batch_size, pretrained_weights=None):\n",
        "        super(NetGlove, self).__init__()\n",
        "        print(\"Num embedding: {}, Embedding dim: {}, Batch size: {}\".format(num_embeddings, embedding_dim, batch_size))\n",
        "        self.batch_size = batch_size\n",
        "        self.embed = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, 64, num_layers=2, dropout=0.5, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(in_features=64, out_features=2)\n",
        "        \n",
        "        if pretrained_weights is not None:\n",
        "            self.embed.weight.data.copy_(pretrained_weights)\n",
        "\n",
        "    def set_initial_weights(self, label):\n",
        "        if label[0] == \"HS\":\n",
        "          vocab = HS_LABEL.vocab\n",
        "        elif label[0] == \"TR\":\n",
        "          vocab = TR_LABEL.vocab\n",
        "        elif label[0] == \"AG\":\n",
        "          vocab = AG_LABEL.vocab\n",
        "\n",
        "        tensor = torch.tensor((1,64), dtype=torch.float)\n",
        "        weights = nn.Parameter(torch.cat((tensor.new_full((1,64), 1/vocab.freqs[\"0\"]), tensor.new_full((1,64), 1/vocab.freqs[\"1\"])), 0), requires_grad=True)\n",
        "        assert weights.shape == self.fc.weight.shape\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.fc.weight = weights\n",
        "  \n",
        "    def forward(self, x):\n",
        "        x = x.transpose(0, 1)\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.embed(x)\n",
        "        x, (h_n, _) = self.lstm(x)\n",
        "        # get the hidden state of the last layer\n",
        "        x = self.fc(h_n[-1])\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irsC2LU5Xq6U",
        "colab_type": "text"
      },
      "source": [
        "## Training and stuffs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaRSCtd5YIwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, criterion, optimizer, train_data, validation_data, vocabs, device, config, params, last_epoch=1, last_max_valid_acc=0, last_min_valid_loss=1000):\n",
        "    model.train()\n",
        "\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    max_valid_acc = last_max_valid_acc\n",
        "    min_valid_loss = last_min_valid_loss\n",
        "\n",
        "    total_train_acc = 0\n",
        "    total_valid_acc = 0\n",
        "    for epoch in range(last_epoch, config.num_epochs + 1):\n",
        "        running_loss = 0\n",
        "        for text, target in train_data:\n",
        "            text = text.to(device) \n",
        "            target = torch.tensor([t[0] for t in target.tolist()], dtype=torch.long).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text)\n",
        "            loss = criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            pred = torch.max(outputs, 1)[1]\n",
        "            train_correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            train_total += target.size(0)\n",
        "            running_loss += loss.item()\n",
        "        epoch_loss = running_loss / len(train_data)\n",
        "        train_accuracy = train_correct.data.cpu().numpy() / train_total\n",
        "        total_train_acc += train_accuracy\n",
        "        # validation after each epoch\n",
        "        validation_accuracy, validation_loss, _ = eval(model, criterion, validation_data, device)\n",
        "        total_valid_acc += validation_accuracy\n",
        "        print(\"Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, Train Acc: {:.3f}%, Validation Acc: {:.3f}%\".format(\n",
        "            epoch, \n",
        "            epoch_loss, \n",
        "            validation_loss,\n",
        "            train_accuracy * 100,\n",
        "            validation_accuracy * 100\n",
        "        ))\n",
        "\n",
        "        if validation_accuracy > max_valid_acc:\n",
        "            model_dict = {\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'train_loss': epoch_loss,\n",
        "                'train_acc': train_accuracy,\n",
        "                'valid_loss': validation_loss,\n",
        "                'valid_acc': validation_accuracy,\n",
        "                'epoch': epoch,\n",
        "                'timestamp': datetime.now().strftime(\"%y/%m/%d %H:%M:%S\"),\n",
        "            }\n",
        "            save_ckpt(model_dict, os.path.join(CKPT_PATH[LANG][\"path\"], params.ckpt_dir), \"model.pt.{:d}\".format(epoch))\n",
        "            max_valid_acc = validation_accuracy\n",
        "        model.train()\n",
        "    print(f\"Total epoch: {config.num_epochs}, Avg Train Acc: {(total_train_acc / config.num_epochs * 100):.3f}%, Avg Valid Acc: {(total_valid_acc / config.num_epochs * 100):.3f}%\")\n",
        "    return model\n",
        "\n",
        "def eval(model, criterion, validation_data, device):\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    gold_labels = []\n",
        "    pred_labels = []\n",
        "    ids = []\n",
        "    with torch.no_grad(): \n",
        "        for data, target in validation_data:\n",
        "            data = data.to(device)\n",
        "            ids += [t[1] for t in target.tolist()]\n",
        "            target = torch.tensor([t[0] for t in target.tolist()], dtype=torch.long).to(device)\n",
        "            output = model(data)\n",
        "            validation_loss += criterion(output, target).item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "            total += target.size(0)\n",
        "            # save labels\n",
        "            gold_labels += list(target.int())\n",
        "            pred_labels += list(pred.data.int().cpu())\n",
        "    validation_loss /= len(validation_data)\n",
        "    return correct.data.cpu().numpy() / total, validation_loss, (gold_labels, pred_labels, ids)\n",
        "\n",
        "def save_ckpt(ckpt, ckpt_dir, ckpt_name):\n",
        "    if not os.path.isdir(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "    torch.save(ckpt, os.path.join(ckpt_dir, ckpt_name))\n",
        "\n",
        "def load_best_model(device, params):\n",
        "    weights_dir = os.path.join(CKPT_PATH[LANG][\"path\"], params.ckpt_dir)\n",
        "    matching_ckpts = [k for k in os.listdir(weights_dir) if\n",
        "                      os.path.isfile(os.path.join(weights_dir, k))]\n",
        "    if not matching_ckpts:\n",
        "        msg = \"No checkpoints found in {}\".format(weights_dir)\n",
        "        if params.load_ckpt == 1:\n",
        "            raise IOError(msg)\n",
        "        print(msg)\n",
        "    else:\n",
        "       matching_ckpts.sort(key=lambda x: [int(c) if c.isdigit() else c for c in re.split(r'(\\d+)', x)])\n",
        "       ckpt_path = os.path.join(weights_dir, matching_ckpts[-1])\n",
        "       print(\"Loading checkpoint from: {}\".format(ckpt_path))\n",
        "       return torch.load(ckpt_path, map_location=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdE9bee-eO37",
        "colab_type": "code",
        "outputId": "fbc2bca3-cc97-462b-b1fb-99fefc9f52d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "class TrainParameters:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        :ivar load_ckpt\n",
        "            0: train from scratch\n",
        "            1: load and test\n",
        "            2: load if exists and continue training\n",
        "        :ivar task\n",
        "            0: HS\n",
        "            1: TR\n",
        "            2: AG\n",
        "        :ivar use_pretrained\n",
        "            True: use pre-trained glove model glove.twitter.27b.200d\n",
        "            False: do not use any pre-trained model\n",
        "        \"\"\"\n",
        "        self.load_ckpt = 0\n",
        "        self.task = 0\n",
        "        self.use_pretrained = True #True\n",
        "        self.use_balanced_weight = False\n",
        "\n",
        "        if self.task == 0:\n",
        "            self.label = [\"HS\", \"id\"]\n",
        "            self.output_name = \"en_a.tsv\"\n",
        "            self.ckpt_dir = \"task_1/\"\n",
        "            \n",
        "        elif self.task == 1:\n",
        "            self.label = [\"TR\", \"id\"]\n",
        "            self.output_name = \"en_b.tsv\"\n",
        "            self.ckpt_dir = \"task_2/\"\n",
        "     \n",
        "        elif self.task == 2:\n",
        "            self.label = [\"AG\", \"id\"]\n",
        "            self.output_name = \"en_c.tsv\"\n",
        "            self.ckpt_dir = \"task_3/\"\n",
        "\n",
        "print(\"CUDA Available: \",torch.cuda.is_available())\n",
        "print(\"GPU Info: {}\".format(torch.cuda.get_device_name()))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# config\n",
        "config = HyperParameters()\n",
        "# training params\n",
        "params = TrainParameters()\n",
        "\n",
        "# load data into dataloader\n",
        "train_data, validation_data, test_data = load_data(lang=LANG, device=device, config=config, label=params.label, use_pretrained=params.use_pretrained)\n",
        "print(len(TEXT.vocab))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available:  True\n",
            "GPU Info: Tesla P100-PCIE-16GB\n",
            "Data Summary: Train: 9000, Validation: 1000, Test: 2805\n",
            "16376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06lBu7OKXpvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if params.use_pretrained and not params.use_balanced_weight:\n",
        "    # only pre-trained weight\n",
        "    config.embedding_dim = 200\n",
        "    model = NetGlove(len(TEXT.vocab), config.embedding_dim, config.batch_size, pretrained_weights=TEXT.vocab.vectors).to(device)\n",
        "elif params.use_balanced_weight and not params.use_pretrained:\n",
        "    # only balanced weight\n",
        "    config.embedding_dim = 128\n",
        "    model = Net(len(TEXT.vocab), config.embedding_dim, config.batch_size, label=params.label).to(device)\n",
        "elif not params.use_balanced_weight and not params.use_pretrained:\n",
        "    # none\n",
        "    config.embedding_dim = 128\n",
        "    model = Net(len(TEXT.vocab), config.embedding_dim, config.batch_size, label=params.label).to(device)\n",
        "else:\n",
        "    raise Exception(\"Not supported!\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "print(\"Train Config Summay\")\n",
        "print(f\"Use pre-trained weight: {params.use_pretrained}, Use balanced weight: {params.use_balanced_weight}, Task: {params.task}, Load ckpt: {params.load_ckpt}\")\n",
        "\n",
        "# load last checkpoint\n",
        "if params.load_ckpt == 1 or params.load_ckpt == 2:\n",
        "    ckpt = load_best_model(device, params)\n",
        "    if ckpt:\n",
        "        model.load_state_dict(ckpt[\"model\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "        last_epoch = int(ckpt[\"epoch\"])\n",
        "        max_valid_acc = ckpt[\"valid_acc\"]\n",
        "        min_valid_loss = ckpt[\"valid_loss\"]\n",
        "        print(max_valid_acc)\n",
        "        print(min_valid_loss)\n",
        "    else:\n",
        "        last_epoch = 0\n",
        "        max_valid_acc = 0\n",
        "\n",
        "if params.load_ckpt == 2:\n",
        "    # continue training\n",
        "    train(model, criterion, optimizer, train_data, validation_data, TEXT.vocab, device, config, params, last_epoch=last_epoch+1, last_max_valid_acc=max_valid_acc, last_min_valid_loss=min_valid_loss)\n",
        "elif params.load_ckpt == 0:\n",
        "    # start from stratch\n",
        "    train(model, criterion, optimizer, train_data, validation_data, TEXT.vocab, device, config, params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v52j23kD_6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Testing...\")\n",
        "start_t = time.time()\n",
        "test_accuracy, test_loss, (gold_labels, pred_labels, ids) = eval(model, criterion, test_data, device)\n",
        "end_t = time.time()\n",
        "test_time = end_t - start_t\n",
        "\n",
        "# convert tensor to regular python datatype\n",
        "ids = list(map(int, ids))\n",
        "gold_labels = list(map(lambda x: x.item(), gold_labels))\n",
        "pred_labels = list(map(lambda x: x.item(), pred_labels))\n",
        "\n",
        "# save output to tsv\n",
        "tsv_path = os.path.join(OUTPUT_PATH[LANG][\"path\"], params.output_name)\n",
        "with open(tsv_path, \"wt\") as output_file:\n",
        "    tsv_writer = csv.writer(output_file, delimiter=\"\\t\")\n",
        "    tsv_writer.writerows(zip(ids, pred_labels))\n",
        "\n",
        "# f1_score = sklearn.metrics.f1_score(gold_labels, pred_labels)\n",
        "precision, recall, f1_score, _ = sklearn.metrics.precision_recall_fscore_support(gold_labels, pred_labels, average='binary')\n",
        "print(f'Test Loss: {test_loss:.6f}, Test Acc: {test_accuracy*100:.3f}%, Test Time: {test_time:.3f} sec')\n",
        "print(f\"F1 Score: {f1_score}, Precision: {precision:.3f}, Recall: {recall:.3f}\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}